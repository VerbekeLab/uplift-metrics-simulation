{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33fe221-d820-4bb9-b066-6346bbb19800",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ccaae-32f0-4fdd-b629-dfcd193d391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import uplift models\n",
    "import torch\n",
    "from catenets.models.torch import TARNet, SNet\n",
    "from econml.metalearners import SLearner, XLearner\n",
    "from econml.dr import LinearDRLearner\n",
    "from econml.dml import CausalForestDML\n",
    "\n",
    "# Datasets\n",
    "from sklift.datasets import fetch_hillstrom\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea253ebb-6b58-4d45-8953-42a6900fc471",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a6210-7652-4490-a8b2-58069f881f96",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5594078-b2a6-44d7-8988-9682751798ac",
   "metadata": {},
   "source": [
    "*METRICS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2065c7-2113-4026-8323-6d9ff36cdf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def safe_divide(numerator, denominator):\n",
    "    return np.where(denominator != 0, numerator / denominator, 0)\n",
    "\n",
    "def metrics_fast(df, perc=10):\n",
    "    df = df.sort_values(by='ITE', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Convert relevant columns to NumPy arrays for faster processing\n",
    "    obs = df['Obs'].values\n",
    "    out = df['Out'].values\n",
    "    not_obs = 1 - obs\n",
    "    not_out = 1 - out\n",
    "\n",
    "    # Calculate cumulative sums using NumPy\n",
    "    cum_N1_T = np.cumsum(obs & out)\n",
    "    cum_N0_T = np.cumsum(obs & not_out)\n",
    "    cum_N1_C = np.cumsum(not_obs & out)\n",
    "    cum_N0_C = np.cumsum(not_obs & not_out)\n",
    "\n",
    "    N_1T = cum_N1_T[-1]\n",
    "    N_0T = cum_N0_T[-1]\n",
    "    N_1C = cum_N1_C[-1]\n",
    "    N_0C = cum_N0_C[-1]\n",
    "    \n",
    "    N_T = N_1T + N_0T\n",
    "    N_C = N_1C + N_0C\n",
    "    L = len(df)\n",
    "\n",
    "    Xphi = np.arange(L + 1) / L\n",
    "    N1_T_over_N_T = N_1T / N_T\n",
    "    N1_C_over_N_C = N_1C / N_C\n",
    "\n",
    "    Qinilist = safe_divide(cum_N1_T, N_T) - safe_divide(cum_N1_C, N_C) - \\\n",
    "               (np.arange(1, L + 1) / L * (N1_T_over_N_T - N1_C_over_N_C))\n",
    "    Qinilist = np.concatenate([[0], Qinilist])\n",
    "    QS = np.trapz(Qinilist, Xphi)\n",
    "\n",
    "    TOCY = safe_divide(cum_N1_T, cum_N1_T + cum_N0_T) - \\\n",
    "           safe_divide(cum_N1_C, cum_N1_C + cum_N0_C) + \\\n",
    "           N1_C_over_N_C - N1_T_over_N_T\n",
    "    TOCY = np.concatenate([[0], TOCY])\n",
    "    TOCS = np.trapz(TOCY, Xphi)\n",
    "\n",
    "    ROCiniY = safe_divide(cum_N1_T, N_1T) - \\\n",
    "              safe_divide(cum_N0_T, N_0T) + \\\n",
    "              safe_divide(cum_N0_C, N_0C) - \\\n",
    "              safe_divide(cum_N1_C, N_1C)\n",
    "    ROCiniY = np.concatenate([[0], ROCiniY])\n",
    "    ROCiniS = np.trapz(ROCiniY, Xphi)\n",
    "\n",
    "    pROCiniX = (safe_divide(cum_N0_T, N_0T) + safe_divide(cum_N1_C, N_1C)) / 2\n",
    "    pROCiniX = np.concatenate([[0], pROCiniX])\n",
    "\n",
    "    pROCiniY = (safe_divide(cum_N1_T, N_1T) + safe_divide(cum_N0_C, N_0C)) / 2\n",
    "    pROCiniY = np.concatenate([[0], pROCiniY])\n",
    "\n",
    "    pROCiniS = np.trapz(pROCiniY, pROCiniX)\n",
    "\n",
    "    pTOCX = (safe_divide(cum_N1_C, cum_N1_C + cum_N0_C)) / N1_C_over_N_C \n",
    "    pTOCX = np.concatenate([[0], pTOCX])\n",
    "\n",
    "    pTOCY = (safe_divide(cum_N1_T, cum_N1_T + cum_N0_T)) /N1_T_over_N_T\n",
    "    pTOCY = np.concatenate([[0], pTOCY])\n",
    "\n",
    "    pTOCS = np.trapz(pTOCY, pTOCX)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    CROCX = safe_divide(cum_N0_T + cum_N1_C, N_0T + N_1C)\n",
    "    CROCX = np.concatenate([[0], CROCX])\n",
    "\n",
    "    CROCY = safe_divide(cum_N1_T + cum_N0_C, N_1T + N_0C)\n",
    "    CROCY = np.concatenate([[0], CROCY])\n",
    "\n",
    "    CROCS = np.trapz(CROCY, CROCX)\n",
    "\n",
    "    cutoff_index = int(L * perc / 100)\n",
    "    df_top_perc = df.iloc[:cutoff_index]\n",
    "\n",
    "    obs_top = df_top_perc['Obs'].values\n",
    "    out_top = df_top_perc['Out'].values\n",
    "    not_obs_top = 1 - obs_top\n",
    "    not_out_top = 1 - out_top\n",
    "\n",
    "    cum_N1_T_10 = np.cumsum(obs_top & out_top)\n",
    "    cum_N0_T_10 = np.cumsum(obs_top & not_out_top)\n",
    "    cum_N1_C_10 = np.cumsum(not_obs_top & out_top)\n",
    "    cum_N0_C_10 = np.cumsum(not_obs_top & not_out_top)\n",
    "\n",
    "    N_1T_10 = cum_N1_T_10[-1]\n",
    "    N_0T_10 = cum_N0_T_10[-1]\n",
    "    N_1C_10 = cum_N1_C_10[-1]\n",
    "    N_0C_10 = cum_N0_C_10[-1]\n",
    "    \n",
    "    N_T_10 = N_1T_10 + N_0T_10\n",
    "    N_C_10 = N_1C_10 + N_0C_10\n",
    "    L_10 = len(df_top_perc)\n",
    "\n",
    "    Xphi_10 = np.arange(L_10 + 1) / L_10\n",
    "    N1_T_over_N_T_10 = safe_divide(N_1T_10, N_T_10)\n",
    "    N1_C_over_N_C_10 = safe_divide(N_1C_10, N_C_10)\n",
    "    Qinilist_10 = safe_divide(cum_N1_T_10, N_T_10) - \\\n",
    "                  safe_divide(cum_N1_C_10, N_C_10) - \\\n",
    "                  (np.arange(1, L_10 + 1) / L * (N1_T_over_N_T_10 - N1_C_over_N_C_10))\n",
    "    Qinilist_10 = np.concatenate([[0], Qinilist_10])\n",
    "    QS10 = np.trapz(Qinilist_10, Xphi_10)\n",
    "\n",
    "    return QS10, QS, TOCS, ROCiniS, pROCiniS, CROCS, pTOCS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f0a58-5733-45f1-beae-321d8fa0a62f",
   "metadata": {},
   "source": [
    "*SYNTHETIC DATA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ad9f29-ef04-469c-a44d-f504480b78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_outcomes(X, T, beta0=0.0, beta_coef=None, beta_t=0.5, sigma =0.1):\n",
    "    \"\"\"\n",
    "    Generate synthetic probabilities and true individual treatment effects.\n",
    "    The probability is defined as:\n",
    "    \n",
    "        p = 1 / (1 + exp( beta0 + X.dot(beta_coef) + beta_t * T) )\n",
    "    \n",
    "    where beta0 is the intercept, beta_coef are the coefficients for features in X,\n",
    "    and beta_t is the treatment effect.\n",
    "    \n",
    "    The true individual treatment effect (ITE) is computed as the difference between the probability\n",
    "    when T=1 and T=0.\n",
    "    \n",
    "    Returns:\n",
    "        p: The computed probabilities.\n",
    "        true_ITE: The true individual treatment effect.\n",
    "    \"\"\"\n",
    "    if beta_coef is None:\n",
    "        beta_coef = np.ones(X.shape[1])\n",
    "    rnd_err = np.random.normal(scale= sigma)\n",
    "    # Compute linear predictor for each unit (incorporating treatment effect)\n",
    "    lin = beta0 + np.dot(X, beta_coef) + beta_t * T + rnd_err\n",
    "    # Compute probability using a sigmoid of v * exp(linear predictor)\n",
    "    p = 1 / (1 +  np.exp(-lin))\n",
    "    \n",
    "    # Compute true ITE: probability difference when T=1 vs T=0\n",
    "    lin_treat = beta0 + np.dot(X, beta_coef) + beta_t + rnd_err # treatment scenario: T=1\n",
    "    lin_control = beta0 + np.dot(X, beta_coef) + rnd_err           # control scenario: T=0\n",
    "    p_treat = 1 / (1 + np.exp(-lin_treat))\n",
    "    p_control = 1 / (1 + np.exp(-lin_control))\n",
    "    true_ITE = p_treat - p_control\n",
    "    \n",
    "    return p, true_ITE, p_treat, p_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa3dc53-a6f5-4eae-b825-631e72360e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define S-Learner functions ---\n",
    "def train_predict_S_lr(X_train, T_train, Y_train, X_test):\n",
    "       # Augment features by appending T as an extra column\n",
    "       X_train_s = np.hstack([X_train, T_train.reshape(-1, 1)])\n",
    "       model = LogisticRegression()\n",
    "       model.fit(X_train_s, Y_train)\n",
    "       # For test set, predict under treatment and control by replacing T with 1 and 0 respectively\n",
    "       X_test_treated = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "       X_test_control = np.hstack([X_test, np.zeros((X_test.shape[0], 1))])\n",
    "       uplift = model.predict_proba(X_test_treated)[:, 1] - model.predict_proba(X_test_control)[:, 1]\n",
    "       return uplift\n",
    "       \n",
    "def train_predict_S_xgb(X_train, T_train, Y_train, X_test):\n",
    "       X_train_s = np.hstack([X_train, T_train.reshape(-1, 1)])\n",
    "       model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "       model.fit(X_train_s, Y_train)\n",
    "       X_test_treated = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "       X_test_control = np.hstack([X_test, np.zeros((X_test.shape[0], 1))])\n",
    "       uplift = model.predict_proba(X_test_treated)[:, 1] - model.predict_proba(X_test_control)[:, 1]\n",
    "       return uplift\n",
    "       \n",
    "   # --- Define T-Learner functions ---\n",
    "def train_predict_T_lr(X_train, T_train, Y_train, X_test):\n",
    "       # Train separate models for treated and control samples\n",
    "       model_treated = LogisticRegression()\n",
    "       model_control = LogisticRegression()\n",
    "       model_treated.fit(X_train[T_train == 1], Y_train[T_train == 1])\n",
    "       model_control.fit(X_train[T_train == 0], Y_train[T_train == 0])\n",
    "       uplift = model_treated.predict_proba(X_test)[:, 1] - model_control.predict_proba(X_test)[:, 1]\n",
    "       return uplift\n",
    "       \n",
    "def train_predict_T_xgb(X_train, T_train, Y_train, X_test):\n",
    "       model_treated = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "       model_control = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "       model_treated.fit(X_train[T_train == 1], Y_train[T_train == 1])\n",
    "       model_control.fit(X_train[T_train == 0], Y_train[T_train == 0])\n",
    "       uplift = model_treated.predict_proba(X_test)[:, 1] - model_control.predict_proba(X_test)[:, 1]\n",
    "       return uplift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba337f9-94a1-4867-96d9-eb591b9ec80d",
   "metadata": {},
   "source": [
    "*EXPERIMENT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85e7492-164d-4348-a2ee-d050e6b9f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_on_dataset(n = 1000):\n",
    "    print(\"Running experiment on Hillstrom dataset\")\n",
    "    X, T = load_dataset()\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = X.values\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, T_train, T_test = train_test_split(X, T, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Normalize training features and apply the same normalization to test features\n",
    "    train_mean = X_train.mean(axis=0)\n",
    "    train_std = X_train.std(axis=0)\n",
    "    X_train = (X_train - train_mean) / train_std\n",
    "    X_test = (X_test - train_mean) / train_std\n",
    "\n",
    "    # Generate synthetic probabilities and true uplift for both sets\n",
    "    p_train, true_ITE_train, _, _ = generate_synthetic_outcomes(X_train, T_train)\n",
    "    p_test, true_ITE_test, _, _ = generate_synthetic_outcomes(X_test, T_test)\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.hist(p_train, bins=30, density=True, edgecolor='k')\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of p_train\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    # print(\"Mean:\", np.mean(p_train))\n",
    "    # print(\"Std:\", np.std(p_train))\n",
    "    # print(\"Min:\", np.min(p_train))\n",
    "    # print(\"Max:\", np.max(p_train))\n",
    "\n",
    "    # Collapse probabilities to discrete outcomes: 1 if p >= 0.5, else 0\n",
    "    Y_train_GT = (p_train >= 0.5).astype(int)\n",
    "    Y_test_GT = (p_test >= 0.5).astype(int)\n",
    "    \n",
    "    # Store our four models (two S-learners and two T-learners) in a dictionary\n",
    "    GT_models = {\n",
    "        'S_LR': train_predict_S_lr,\n",
    "        'T_LR': train_predict_T_lr,\n",
    "        'S_XGB': train_predict_S_xgb,\n",
    "        'T_XGB': train_predict_T_xgb\n",
    "    }\n",
    "    #Get ground truth of models ranking\n",
    "\n",
    "    # Create a dictionary to store the predicted ITEs and ranking scores for each model.\n",
    "    GT_ranking_results = {}\n",
    "    print(p_train)\n",
    "    for name, func in GT_models.items():\n",
    "        print(f\"Training model: {name}\")\n",
    "        try:\n",
    "            # Train model on (X_train, T_train, p_train) and get predicted uplift on X_test\n",
    "            predicted_ITE = func(X_train, T_train, Y_train_GT, X_test)\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model {name}: {e}\")\n",
    "            predicted_ITE = np.zeros(len(T_test))\n",
    "        \n",
    "        # Compute a ranking distance metric between the predicted ITE ranking and the ground truth ranking.\n",
    "        # Here we use Spearman's rank correlation coefficient.\n",
    "        rho, _ = spearmanr(predicted_ITE, true_ITE_test)\n",
    "        # A higher Spearman correlation means a better match.\n",
    "        \n",
    "        GT_ranking_results[name] = {\n",
    "            'ranking_score': rho,\n",
    "            'predicted_ITE': predicted_ITE\n",
    "        }\n",
    "        print(f\"Ranking for {name}: Spearman rho = {rho:.4f}\")\n",
    "    # print(GT_ranking_results)\n",
    "\n",
    "    # At this point, ranking_results contains each model's predicted ITEs and their ranking score.\n",
    "    # Compute metrics for GT ranking.\n",
    "    \n",
    "    # Now draw binary outcomes based on the computed probabilities\n",
    "    Y_train = np.random.binomial(1, p_train)\n",
    "    trained_models_uplifts = {\n",
    "        'S_LR': train_predict_S_lr,\n",
    "        'T_LR': train_predict_T_lr,\n",
    "        'S_XGB': train_predict_S_xgb,\n",
    "        'T_XGB': train_predict_T_xgb\n",
    "    }\n",
    "        \n",
    "    for name, func in trained_models_uplifts.items():\n",
    "        print(f\"Training model: {name}\")\n",
    "        try:\n",
    "            # Train model on (X_train, T_train, p_train) and get predicted uplift on X_test\n",
    "            uplift_pred = func(X_train, T_train, Y_train, X_test)\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model {name}: {e}\")\n",
    "            predicted_ITE = np.zeros(len(T_test))\n",
    "        trained_models_uplifts[name] = uplift_pred\n",
    "\n",
    "    results = {}\n",
    "    for i in range(n):\n",
    "        #sanity loop\n",
    "        if i % (n/10) == 0:\n",
    "            print(\"iteration:\", i)\n",
    "        Y_test = np.random.binomial(1, p_test)\n",
    "        results_i = {}\n",
    "\n",
    "        df_GT = pd.DataFrame({\n",
    "            'Obs': T_test.astype(int),\n",
    "            'Out': Y_test,\n",
    "            'ITE': true_ITE_test\n",
    "        })\n",
    "\n",
    "        metrics_GT = metrics_fast(df_GT)\n",
    "        results_i[\"GT\"] = {\n",
    "                'QS10': metrics_GT[0],\n",
    "                'QS': metrics_GT[1],\n",
    "                'TOCS': metrics_GT[2],\n",
    "                'ROCiniS': metrics_GT[3],\n",
    "                'pROCiniS': metrics_GT[4],\n",
    "                'CROCS': metrics_GT[5],\n",
    "                'pTOCS': metrics_GT[6]\n",
    "            }\n",
    "    \n",
    "        for name, uplift_pred in trained_models_uplifts.items():\n",
    "            # Prepare DataFrame for metric evaluation\n",
    "            df_eval = pd.DataFrame({\n",
    "                'Obs': T_test.astype(int),\n",
    "                'Out': Y_test,\n",
    "                'ITE': uplift_pred\n",
    "            })\n",
    "            metrics = metrics_fast(df_eval)\n",
    "            results_i[name] = {\n",
    "                'QS10': metrics[0],\n",
    "                'QS': metrics[1],\n",
    "                'TOCS': metrics[2],\n",
    "                'ROCiniS': metrics[3],\n",
    "                'pROCiniS': metrics[4],\n",
    "                'CROCS': metrics[5],\n",
    "                'pTOCS': metrics[6]\n",
    "            }\n",
    "            #print(f\"Metrics for {name}: {results[name]}\")\n",
    "            results[i] = results_i\n",
    "            \n",
    "    return results, GT_ranking_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984195e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(sample_size=1000):\n",
    "    \"\"\"\n",
    "    Load the Hillstrom dataset from sklift.\n",
    "    Returns features X (raw, with categorical variables converted to dummies)\n",
    "    and a binary treatment indicator T.\n",
    "    The treatment indicator is set to 1 if an E-Mail was sent (\"Mens E-Mail\" or \"Womens E-Mail\")\n",
    "    and 0 if \"No E-Mail\".\n",
    "    \"\"\"\n",
    "    dataset = fetch_hillstrom()\n",
    "    df = dataset['data']\n",
    "    if len(df) > sample_size:\n",
    "        df = df.sample(sample_size, random_state=42)\n",
    "        df = df.sample(sample_size)\n",
    "    # Convert categorical features into dummy variables and cast to float\n",
    "    X = pd.get_dummies(df, drop_first=True).astype(float)\n",
    "    # Subsample the treatment indicator using the same indices as the data\n",
    "    T = dataset['treatment'].loc[df.index]\n",
    "    T = (T != \"No E-Mail\").astype(int).values\n",
    "    return X, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee4fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr, kendalltau, wasserstein_distance\n",
    "def weighted_kendall_tau(x, y, w=None):\n",
    "    n = len(x)\n",
    "    assert len(y) == n\n",
    "    if w is None:\n",
    "        w = np.abs(np.subtract.outer(y, y))\n",
    "    num = 0.0\n",
    "    denom = 0.0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = np.sign(x[i] - x[j])\n",
    "            b = np.sign(y[i] - y[j])\n",
    "            weight = w[i, j]\n",
    "            num += weight * (a != b)\n",
    "            denom += weight\n",
    "    return 1 - (num / denom) if denom != 0 else np.nan\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_and_predict_ites():\n",
    "    print(\"Running experiment on Hillstrom dataset\")\n",
    "    \n",
    "    X, T = load_dataset()\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = X.values\n",
    "    \n",
    "    X_train, X_test, T_train, T_test = train_test_split(X, T, test_size=0.3, random_state=42)\n",
    "\n",
    "    train_mean = X_train.mean(axis=0)\n",
    "    train_std = X_train.std(axis=0)\n",
    "    X_train = (X_train - train_mean) / train_std\n",
    "    X_test = (X_test - train_mean) / train_std\n",
    "\n",
    "    p_train, true_ITE_train, _, _ = generate_synthetic_outcomes(X_train, T_train)\n",
    "    p_test, true_ITE_test, p_treat_test, p_control_test = generate_synthetic_outcomes(X_test, T_test)\n",
    "\n",
    "    Y_train_GT = (p_train >= 0.5).astype(int)\n",
    "\n",
    "    GT_models = {\n",
    "        'S_LR': train_predict_S_lr,\n",
    "        'T_LR': train_predict_T_lr,\n",
    "        'S_XGB': train_predict_S_xgb,\n",
    "        'T_XGB': train_predict_T_xgb\n",
    "    }\n",
    "\n",
    "    predicted_ites_dict = {}\n",
    "    for name, func in GT_models.items():\n",
    "        print(f\"Training model: {name}\")\n",
    "        try:\n",
    "            predicted_ITE = func(X_train, T_train, Y_train_GT, X_test)\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model {name}: {e}\")\n",
    "            predicted_ITE = np.zeros(len(T_test))\n",
    "        predicted_ites_dict[name] = predicted_ITE\n",
    "\n",
    "    df_ITES = pd.DataFrame(predicted_ites_dict)\n",
    "    df_ITES[\"true_ITE\"] = true_ITE_test\n",
    "    df_ITES[\"PT\"] = p_treat_test\n",
    "    df_ITES[\"PC\"] = p_control_test\n",
    "\n",
    "\n",
    "    # Uniform histogram settings\n",
    "    bins = 40\n",
    "    min_val = df_ITES.min().min()\n",
    "    max_val = df_ITES.max().max()\n",
    "    y_max = 0\n",
    "    hist_data = {}\n",
    "    model_cols = [col for col in df_ITES.columns if col not in {\"PT\", \"PC\"}]\n",
    "    for col in model_cols:\n",
    "        counts, bin_edges = np.histogram(df_ITES[col], bins=bins, range=(min_val, max_val), density=True)\n",
    "        hist_data[col] = counts\n",
    "        y_max = max(y_max, counts.max())\n",
    "    for col in model_cols:\n",
    "        plt.hist(df_ITES[col], bins=bins, range=(min_val, max_val), density=True, edgecolor='k')\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xlabel(\"ITE\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.xlim(min_val, max_val)\n",
    "        plt.ylim(0, y_max * 1.05)\n",
    "        plt.show()\n",
    "\n",
    "    # Metric computation\n",
    "    results = {}\n",
    "    for col in model_cols:\n",
    "        if col == \"true_ITE\":\n",
    "            continue\n",
    "        x = df_ITES[col].values\n",
    "        y = df_ITES[\"true_ITE\"].values\n",
    "\n",
    "        mse = np.mean((x - y) ** 2)\n",
    "        rho, _ = spearmanr(x, y)\n",
    "        tau, _ = kendalltau(x, y)\n",
    "        w_kendall = weighted_kendall_tau(x, y)\n",
    "        emd = wasserstein_distance(x, y)\n",
    "\n",
    "        rank_x = pd.Series(x).rank().values\n",
    "        rank_y = pd.Series(y).rank().values\n",
    "        mean_rank_dist = np.mean(np.abs(rank_x - rank_y))\n",
    "\n",
    "        results[col] = {\n",
    "            'MSE': mse,\n",
    "            'Spearman_rho': rho,\n",
    "            'Kendall_tau': tau,\n",
    "            'Weighted_Kendall': w_kendall,\n",
    "            'EMD': emd,\n",
    "            'Mean_Rank_Dist': mean_rank_dist\n",
    "        }\n",
    "\n",
    "    df_metrics = pd.DataFrame(results).T\n",
    "\n",
    "    def color_ranks(s, maximize=True):\n",
    "        order = s.rank(ascending=not maximize, method=\"min\")\n",
    "        colors = []\n",
    "        for r in order:\n",
    "            if r == 1:\n",
    "                colors.append('background-color: #228B22')  # light green\n",
    "            elif r == 2:\n",
    "                colors.append('background-color: #1E90FF')  # light blue\n",
    "            elif r == 3:\n",
    "                colors.append('background-color: #9370DB')  # light purple\n",
    "            else:\n",
    "                colors.append('')\n",
    "        return colors\n",
    "\n",
    "    styled = (\n",
    "        df_metrics.style\n",
    "        .apply(color_ranks, subset=['Spearman_rho', 'Kendall_tau', 'Weighted_Kendall'], maximize=True)\n",
    "        .apply(color_ranks, subset=['MSE', 'EMD', 'Mean_Rank_Dist'], maximize=False)\n",
    "        .format(precision=4)\n",
    "    )\n",
    "    \n",
    "    display(styled)\n",
    "    display(df_ITES)\n",
    "    return df_ITES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41805fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=prepare_dataset_and_predict_ites()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_two_columns = df.iloc[:, -2:]\n",
    "\n",
    "# Count zeros and ones\n",
    "count_zeros = (last_two_columns <0.01).sum().sum()\n",
    "count_ones = (last_two_columns >0.99).sum().sum()\n",
    "\n",
    "print(\"Number of zeros in last 2 columns:\", count_zeros)\n",
    "print(\"Number of ones in last 2 columns:\", count_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d64a9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def simulate_and_score_uplift_metrics(df_ITES, n_runs=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Runs the uplift simulation and evaluation n_runs times.\n",
    "    Returns:\n",
    "        metrics_dict: {model_name: {metric_name: [v1, v2, ..., v_n_runs], ...}, ...}\n",
    "    \"\"\"\n",
    "    uplift_cols = ['S_LR', 'T_LR', 'S_XGB', 'T_XGB', 'true_ITE']\n",
    "    metric_names = ['QS10', 'QS', 'TOCS', 'ROCiniS', 'pROCiniS', 'CROCS', 'pTOCS']\n",
    "\n",
    "    # Initialize result structure\n",
    "    metrics_dict = {\n",
    "        model: {metric: [] for metric in metric_names}\n",
    "        for model in uplift_cols\n",
    "    }\n",
    "\n",
    "    for run in tqdm(range(n_runs)):\n",
    "        N = len(df_ITES)\n",
    "        PT = df_ITES[\"PT\"].values\n",
    "        PC = df_ITES[\"PC\"].values\n",
    "\n",
    "        Obs = np.random.binomial(1, 0.5, N)\n",
    "        OutT = np.random.binomial(1, PT)\n",
    "        OutC = np.random.binomial(1, PC)\n",
    "        Out = np.where(Obs == 1, OutT, OutC)\n",
    "\n",
    "        df_aug = df_ITES.copy()\n",
    "        df_aug[\"Obs\"] = Obs\n",
    "        df_aug[\"OutT\"] = OutT\n",
    "        df_aug[\"OutC\"] = OutC\n",
    "        df_aug[\"Out\"] = Out\n",
    "\n",
    "        for col in uplift_cols:\n",
    "            df_eval = df_aug[['Obs', 'Out', col]].copy()\n",
    "            df_eval.columns = ['Obs', 'Out', 'ITE']\n",
    "\n",
    "            try:\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    QS10, QS, TOCS, ROCiniS, pROCiniS, CROCS, _ = metrics_fast(df_eval)\n",
    "                    values = [QS10, QS, TOCS, ROCiniS, pROCiniS, CROCS]\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"[ERROR] Run {run}, model {col}\")\n",
    "                    print(df_eval.head())\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                values = [np.nan] * 7\n",
    "\n",
    "            for name, val in zip(metric_names, values):\n",
    "                metrics_dict[col][name].append(val)\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list= simulate_and_score_uplift_metrics(df,1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bfca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "46b74f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ordering_consistency_table(metrics_dict):\n",
    "    \"\"\"\n",
    "    Evaluates how often the expected pairwise ordering holds across n_runs.\n",
    "\n",
    "    Input:\n",
    "        metrics_dict: nested dict of form\n",
    "            { model: { metric: [v1, ..., vn] } }\n",
    "\n",
    "    Output:\n",
    "        DataFrame with metrics as rows and columns:\n",
    "            'true_ITE > S_LR',\n",
    "            'S_LR > S_XGB',\n",
    "            'S_XGB > T_LR',\n",
    "            'T_LR > T_XGB'\n",
    "        Values are counts over n_runs\n",
    "    \"\"\"\n",
    "    expected_order = ['true_ITE', 'S_LR',  'T_LR','S_XGB', 'T_XGB']\n",
    "    comparisons = [\n",
    "        ('true_ITE', 'S_LR'),\n",
    "        ('S_LR', 'T_LR'),\n",
    "        ('T_LR','S_XGB'),\n",
    "        ('S_XGB', 'T_XGB')\n",
    "    ]\n",
    "    comparison_labels = [\n",
    "        'true_ITE > S_LR',\n",
    "        'S_LR > T_LR',\n",
    "        'T_LR > S_XGB ',\n",
    "        'S_XGB > T_XGB'\n",
    "    ]\n",
    "\n",
    "    metric_names = list(next(iter(metrics_dict.values())).keys())\n",
    "    n_runs = len(next(iter(metrics_dict.values())).values().__iter__().__next__())\n",
    "    counts = {metric: [0, 0, 0, 0] for metric in metric_names}\n",
    "\n",
    "    for metric in metric_names:\n",
    "        for i, (a, b) in enumerate(comparisons):\n",
    "            a_vals = metrics_dict[a][metric]\n",
    "            b_vals = metrics_dict[b][metric]\n",
    "            for va, vb in zip(a_vals, b_vals):\n",
    "                if not (np.isnan(va) or np.isnan(vb)) and va > vb:\n",
    "                    counts[metric][i] += 1\n",
    "\n",
    "\n",
    "    df_result = pd.DataFrame.from_dict(counts, orient='index', columns=comparison_labels) / n_runs * 100\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "0f617d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ordering = evaluate_ordering_consistency_table(metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ordering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
